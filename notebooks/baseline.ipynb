{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export WANDB_API_KEY=440f1ee11bc00f4104c20e09df4fd8a0f51d1924"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple, Union, List, Callable, Any\n",
    "\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import model_selection\n",
    "from sklearn import svm\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB, CategoricalNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import functools\n",
    "from functools import partial\n",
    "import itertools\n",
    "\n",
    "from pytorch_lightning.utilities.apply_func import apply_to_collection\n",
    "from torch import Tensor\n",
    "\n",
    "import mne\n",
    "import pywt\n",
    "\n",
    "\n",
    "import wandb\n",
    "\n",
    "try:\n",
    "    from dask.distributed import Client, progress\n",
    "    import dask.bag as db\n",
    "\n",
    "    # Make it True\n",
    "    using_dask = False\n",
    "except:\n",
    "    using_dask = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# /Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /Data\n",
    "\n",
    "\n",
    "class KlinikDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        eeg_electrode_positions: Dict[str, Tuple[int, int]],\n",
    "        data_path: str,\n",
    "        meta_data=None,\n",
    "        length=1,\n",
    "        transforms=None,\n",
    "    ):\n",
    "\n",
    "        self.eeg_electrode_positions = eeg_electrode_positions\n",
    "        self.data_path = data_path\n",
    "\n",
    "        if meta_data is None:\n",
    "            self.meta_data = pd.read_csv(os.path.join(self.data_path, \"meta_data.csv\"))\n",
    "        else:\n",
    "            self.meta_data = meta_data\n",
    "\n",
    "        df_label_1 = self.meta_data[self.meta_data.label.eq(1)]\n",
    "        df_label_0 = self.meta_data[self.meta_data.label.eq(0)]\n",
    "\n",
    "        self.meta_data = shuffle(\n",
    "            pd.concat(\n",
    "                (\n",
    "                    df_label_0.sample(n=len(df_label_1), replace=False),\n",
    "                    df_label_1,\n",
    "                ),\n",
    "                axis=0,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.meta_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        self.labels = self.meta_data[\"label\"]\n",
    "\n",
    "        self.length = length\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"KlinikDataset\"\n",
    "\n",
    "    def get_class_distribution(self):\n",
    "        return self.meta_data[\"label\"].value_counts().to_list()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta_data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Union[dict, torch.Tensor]:\n",
    "\n",
    "        meta_data = self.meta_data.iloc[idx]\n",
    "\n",
    "        # shape -> (num_channels, n_times)\n",
    "        # eeg_data = np.load(\n",
    "        #     os.path.join(self.data_path, os.path.split(meta_data[\"file_pathe\"])[1])\n",
    "        # )\n",
    "\n",
    "        eeg_data = np.load(os.path.join(\"/\", meta_data[\"file_pathe\"]))\n",
    "\n",
    "        # get the first 21 channels\n",
    "        eeg_data = eeg_data[:21, :]\n",
    "\n",
    "        label = int(meta_data[\"label\"])\n",
    "\n",
    "        wav = {\n",
    "            key: np.expand_dims(eeg_data[i], axis=0)\n",
    "            for i, key in enumerate(self.eeg_electrode_positions.keys())\n",
    "        }\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            wav, label = self.transforms(wav, label)\n",
    "\n",
    "        return wav, label\n",
    "\n",
    "    def get_class(self, index):\n",
    "        return self.meta_data.iloc[index][\"label\"]\n",
    "\n",
    "    def subset(self, indices):\n",
    "        return self.__class__(\n",
    "            eeg_electrode_positions=self.eeg_electrode_positions,\n",
    "            data_path=self.data_path,\n",
    "            meta_data=self.meta_data.iloc[indices],\n",
    "            length=self.length,\n",
    "            transforms=self.transforms,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        imgs = {\n",
    "            key: torch.vstack([item[0][key].unsqueeze(0) for item in batch])\n",
    "            for key in batch[0][0].keys()\n",
    "        }\n",
    "        trgts = torch.vstack([item[1] for item in batch]).squeeze()\n",
    "\n",
    "        return [imgs, trgts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## /audio_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, data, label):\n",
    "        data = apply_to_collection(\n",
    "            data,\n",
    "            dtype=(np.ndarray, int, float, np.int64),\n",
    "            function=lambda a: torch.tensor(a).float(),\n",
    "        )\n",
    "        label = apply_to_collection(\n",
    "            label,\n",
    "            dtype=(np.ndarray, int, float, np.int64),\n",
    "            function=lambda a: torch.tensor(a).float(),\n",
    "        )\n",
    "\n",
    "        return data, label\n",
    "\n",
    "\n",
    "class LabelToDict(object):\n",
    "    def __call__(self, data, label):\n",
    "        return data, {\"label\": label}\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"LabelToDict\"\n",
    "\n",
    "\n",
    "class ZNorm(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        stats: str,\n",
    "        mode: str = \"min-max\",\n",
    "        max_clip_val: int = 0,\n",
    "        min_clip_val: int = None,\n",
    "    ):\n",
    "        self.stats_name = stats\n",
    "        self.mode = mode\n",
    "        self.min_clip_val = min_clip_val if min_clip_val is not None else -max_clip_val\n",
    "        self.max_clip_val = max_clip_val\n",
    "        with open(stats, \"rb\") as stats_f:\n",
    "            self.stats = pickle.load(stats_f)\n",
    "\n",
    "    def __call__(self, pkg: Tuple[Dict[str, Tensor], List[int]], target: Any):\n",
    "        for k, st in self.stats.items():\n",
    "            if k in pkg:\n",
    "                if self.mode == \"min-max\":\n",
    "                    minx = st[\"min\"].unsqueeze(0).to(pkg[k].device)\n",
    "                    maxx = st[\"max\"].unsqueeze(0).to(pkg[k].device)\n",
    "                    pkg[k] = (pkg[k] - minx) / (maxx - minx)\n",
    "                if self.mode == \"mean-std\":\n",
    "                    mean = st[\"mean\"].unsqueeze(0).to(pkg[k].device)\n",
    "                    std = st[\"std\"].unsqueeze(0).to(pkg[k].device)\n",
    "                    pkg[k] = (pkg[k] - mean) / std\n",
    "                if self.max_clip_val > 0 or self.min_clip_val is not None:\n",
    "                    pkg[k] = torch.clip(\n",
    "                        pkg[k], min=self.min_clip_val, max=self.max_clip_val\n",
    "                    )\n",
    "            else:\n",
    "                raise ValueError(f\"couldn't find stats key {k} in package\")\n",
    "        return pkg, target\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms: List[Callable]) -> None:\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, data: Any, target: Any):\n",
    "        for t in self.transforms:\n",
    "            data, target = t(data, target)\n",
    "        return data, target\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Compose(\" + f\"-\".join(str(t) for t in self.transforms) + \")\"\n",
    "\n",
    "\n",
    "class FlattenChannel(object):\n",
    "    def __call__(self, data: Any, target: Any):\n",
    "        for i, worker in enumerate(target.keys()):\n",
    "\n",
    "            if worker == \"label\":\n",
    "                pass\n",
    "            else:\n",
    "                flatten_worker = []\n",
    "                for j, ch in enumerate(target[worker].keys()):\n",
    "                    flatten_worker.append(target[worker][ch])\n",
    "\n",
    "                target[worker] = np.array(flatten_worker)\n",
    "                # print(f\"target flattened : {target[worker].shape}\")\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"FlattenChannel\"\n",
    "\n",
    "\n",
    "class ConcatenateWorker(object):\n",
    "    def __init__(self, transforms: List[Callable]) -> None:\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, data: Any, target: Any):\n",
    "\n",
    "        concatenate_worker = []\n",
    "\n",
    "        for i, worker in enumerate(target.keys()):\n",
    "            if worker == \"label\":\n",
    "                pass\n",
    "            else:\n",
    "                # print(f\"worker : {target[worker].shape}\")\n",
    "                concatenate_worker.append(target[worker])\n",
    "\n",
    "        target[\"concat\"] = np.concatenate(concatenate_worker, axis=-1)\n",
    "\n",
    "        return data, target\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"ConcatenateWorker({'-'.join(str(t) for t in self.transforms)})\"\n",
    "\n",
    "\n",
    "class WTE(object):\n",
    "    def __init__(self, level=4, wavelet=\"db1\", name=\"wte\"):\n",
    "        self.level = level\n",
    "        self.wavelet = wavelet\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        data: Dict[str, np.ndarray],\n",
    "        label: Dict[str, Union[Any, Dict[str, np.ndarray]]],\n",
    "    ):\n",
    "\n",
    "        label[self.name] = apply_to_collection(\n",
    "            data,\n",
    "            dtype=np.ndarray,\n",
    "            function=partial(\n",
    "                self.wavelet_transform_energy, level=self.level, wavelet=self.wavelet\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return data, label\n",
    "\n",
    "    def __repr__(self):\n",
    "        attrs = \"(level={}, wavelet={})\".format(self.level, self.wavelet)\n",
    "        return self.__class__.__name__ + attrs\n",
    "\n",
    "    @staticmethod\n",
    "    def wavelet_transform_energy(signal: np.ndarray, level: int, wavelet: str = \"db1\"):\n",
    "        \"\"\"calculates wavelet transform energy of a 1d signal\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        signal : numpy.ndarray\n",
    "            raw signal. (eg. audio signal)\n",
    "        level : int\n",
    "            wavelet transform maximum level\n",
    "        wavelet : str, optional\n",
    "            wavelet type. one of the type available in :code:`pywt.wavelist()`,\n",
    "             by default \"db1\"\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            The energy vector with shape of (level + 1,)\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The WT energy can be calculated in different ways. Here we have implemented\n",
    "        the method proposed in [1] equation (1) and [2] equation (2):\n",
    "\n",
    "        .. math::\n",
    "            \\tilde{\\mathbf{E}}_{\\mathbf{V}_{j}} =\n",
    "            \\frac{\\sum_{n} (\\mathbf{w}_{j,n})^2}{\\sum_{j=1}^{J_{max}} \\sum_{n} (\\mathbf{w}_{j,n})^2}\n",
    "\n",
    "        Where :math:`\\mathbf{w}_{j,n}` are the coefficients generated by DWT at the\n",
    "        jth decomposition level.\n",
    "\n",
    "        .. [1] K. Qian et al., “A bag of wavelet features for snore sound classification,”\n",
    "           Ann. Biomed. Eng., vol. 47, no. 4, pp. 1000–1011, 2019.\n",
    "        .. [2] Qian, K., C. Janott, Z. Zhang, C. Heiser, and B. Schuller.\n",
    "           Wavelet features for classification of VOTE snore sounds.\n",
    "           In: Proceedings of ICASSP, Shanghai, China, 2016, pp.221–225.\n",
    "        \"\"\"\n",
    "        wt = pywt.wavedec(data=signal, wavelet=wavelet, mode=\"symmetric\", level=level)\n",
    "\n",
    "        ps_wt = np.array([np.sum(np.power(wt_j, 2)) for wt_j in wt])\n",
    "        energy_vector = ps_wt / np.sum(ps_wt)\n",
    "\n",
    "        return energy_vector\n",
    "\n",
    "\n",
    "class WPTE(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        level=4,\n",
    "        wavelet=\"db1\",\n",
    "        include_raw=True,\n",
    "        name=\"wpte\",\n",
    "    ):\n",
    "        self.level = level\n",
    "        self.wavelet = wavelet\n",
    "        self.include_raw = include_raw\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        data: Dict[str, np.ndarray],\n",
    "        label: Dict[str, Union[Any, Dict[str, np.ndarray]]],\n",
    "    ):\n",
    "\n",
    "        label[self.name] = apply_to_collection(\n",
    "            data,\n",
    "            dtype=np.ndarray,\n",
    "            function=partial(\n",
    "                self.wavelet_packet_transform_energy,\n",
    "                maxlevel=self.level,\n",
    "                wavelet=self.wavelet,\n",
    "                include_raw=self.include_raw,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return data, label\n",
    "\n",
    "    def __repr__(self):\n",
    "        attrs = \"(level={}, wavelet={})\".format(self.level, self.wavelet)\n",
    "        return self.__class__.__name__ + attrs\n",
    "\n",
    "    @staticmethod\n",
    "    def wavelet_packet_energy(wpt_subspace: np.ndarray):\n",
    "        \"\"\"calculates the energy of a single subband from subspaces given by wpt\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        wpt_subspace : numpy.ndarray\n",
    "            coefficients calculated by WPT from the signal at subspace V\n",
    "            which is the kth subband at jth level.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The energy of WPT sub space\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The WPT energy can be calculated in different ways. Here we have implemented\n",
    "        the method proposed in [1] equation (2):\n",
    "\n",
    "        .. math:: \\tilde{\\mathbf{E}}_{\\mathbf{V}_{j,k}} = log(\\sqrt{\\frac{\\sum_{n=1}^{N_{j,k}} (\\mathbf{w}_{j,k,n})^2}{N_{j,k}}})\n",
    "\n",
    "        Where :math:`\\mathbf{w}_{j,k,n}` represents the coefficients calculated by\n",
    "        WPT from the signal at the subspace :math:`\\mathbf{V}_{j,k}`.\n",
    "        :math:`N_{j,k}` is the total number of wavelet coefficients in the kth subband\n",
    "        at the jth level.\n",
    "\n",
    "        .. [1] K. Qian et al., “A bag of wavelet features for snore sound classification,”\n",
    "           Ann. Biomed. Eng., vol. 47, no. 4, pp. 1000–1011, 2019.\n",
    "        \"\"\"\n",
    "        energy = np.log(\n",
    "            np.sqrt(np.sum(np.power(wpt_subspace, 2) / wpt_subspace.shape[0]))\n",
    "        )\n",
    "        return energy\n",
    "\n",
    "    @staticmethod\n",
    "    def wavelet_packet_transform_energy(\n",
    "        signal: np.ndarray,\n",
    "        maxlevel: int,\n",
    "        wavelet: str = \"db1\",\n",
    "        include_raw: bool = True,\n",
    "    ):\n",
    "        \"\"\"calculates wavelet packet transform energy of a 1d signal\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        signal : numpy.ndarray\n",
    "            raw signal. (eg. audio signal)\n",
    "        maxlevel : int\n",
    "            wavelet packet transform maximum level\n",
    "        wavelet : str, optional\n",
    "            wavelet type. one of the types available in :code:`pywt.wavelist()`,\n",
    "             by default \"db1\"\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            The energy vector with shape of (:math:`2^{maxlevel + 1} - 1`,)\n",
    "        \"\"\"\n",
    "        energy_vector = []\n",
    "\n",
    "        wp = pywt.WaveletPacket(\n",
    "            data=signal, wavelet=wavelet, mode=\"symmetric\", maxlevel=maxlevel\n",
    "        )\n",
    "        if include_raw:\n",
    "            energy = WPTE.wavelet_packet_energy(signal)\n",
    "            energy_vector.append(energy)\n",
    "\n",
    "        for row in range(1, maxlevel + 1):\n",
    "            for i in [node.path for node in wp.get_level(row, \"freq\")]:\n",
    "                energy = WPTE.wavelet_packet_energy(wp[i].data)\n",
    "                energy_vector.append(energy)\n",
    "\n",
    "        return np.array(energy_vector)\n",
    "\n",
    "\n",
    "class PSD(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sfreq=256,\n",
    "        fmin=0,\n",
    "        fmax=np.inf,\n",
    "        n_fft=256,\n",
    "        n_overlap=128,\n",
    "        n_per_seg=256,\n",
    "        average=\"mean\",\n",
    "        verbose=0,\n",
    "        windowed=False,\n",
    "        unit=\"Hz\",  # Can be bin\n",
    "        name=\"psd\",\n",
    "    ):\n",
    "        self.sfreq = sfreq\n",
    "        self.fmin = fmin\n",
    "        self.fmax = fmax\n",
    "        self.n_fft = n_fft\n",
    "        self.n_overlap = n_overlap\n",
    "        self.n_per_seg = n_per_seg\n",
    "        self.average = average\n",
    "        self.windowed = windowed\n",
    "        self.unit = unit\n",
    "        self.verbose = verbose\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        data: Dict[str, np.ndarray],\n",
    "        label: Dict[str, Union[Any, Dict[str, np.ndarray]]],\n",
    "    ):\n",
    "\n",
    "        label[self.name] = apply_to_collection(\n",
    "            data,\n",
    "            dtype=np.ndarray,\n",
    "            function=partial(\n",
    "                self.power_spectral_density,\n",
    "                sfreq=self.sfreq,\n",
    "                fmin=self.fmin,\n",
    "                fmax=self.fmax,\n",
    "                n_fft=self.n_fft,\n",
    "                n_overlap=self.n_overlap,\n",
    "                n_per_seg=self.n_per_seg,\n",
    "                average=self.average,\n",
    "                verbose=self.verbose,\n",
    "                windowed=self.windowed,\n",
    "                unit=self.unit,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return data, label\n",
    "\n",
    "    def __repr__(self):\n",
    "        attrs = \"(sfreq={}, n_fft={}, n_overlap={}, n_per_seg={})\".format(\n",
    "            self.sfreq, self.n_fft, self.n_overlap, self.n_per_seg\n",
    "        )\n",
    "        return self.__class__.__name__ + attrs\n",
    "\n",
    "    @staticmethod\n",
    "    def power_spectral_density(\n",
    "        signal: np.ndarray,\n",
    "        sfreq: int = 256,\n",
    "        fmin: float = 0,\n",
    "        fmax: float = np.inf,\n",
    "        n_fft: int = 256,\n",
    "        n_overlap: int = 128,\n",
    "        n_per_seg: int = 256,\n",
    "        average: str = \"mean\",\n",
    "        verbose: int = 0,\n",
    "        windowed: bool = False,\n",
    "        unit: str = \"Hz\",\n",
    "    ):\n",
    "\n",
    "        if windowed:\n",
    "            average = None\n",
    "\n",
    "        data, _ = mne.time_frequency.psd_array_welch(\n",
    "            signal,\n",
    "            sfreq=sfreq,\n",
    "            fmin=fmin,\n",
    "            fmax=fmax,\n",
    "            n_fft=n_fft,\n",
    "            n_overlap=n_overlap,\n",
    "            n_per_seg=n_per_seg,\n",
    "            average=average,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        if not windowed:\n",
    "            data = (10 * np.log10(data * sfreq / n_fft)).flatten()\n",
    "            # data = data.flatten()\n",
    "\n",
    "        # The Shape of data should be [trial, window, psds]\n",
    "        # Handle Window data\n",
    "        if unit == \"bin\" and windowed:\n",
    "            data = np.apply_along_axis(\n",
    "                lambda x: 10 * np.log10(x * sfreq / n_fft),\n",
    "                axis=2,\n",
    "                arr=np.transpose(data, (0, 2, 1)),\n",
    "            )\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_transforms(worker_configs):\n",
    "    transforms = [LabelToDict()]\n",
    "    worker_transform = []\n",
    "    # transforms = []\n",
    "    for worker in worker_configs:\n",
    "        name = worker[\"name\"]\n",
    "        if name == \"wte\":\n",
    "            transforms.append(WTE())\n",
    "            worker_transform.append(WTE())\n",
    "        elif name == \"wpte\":\n",
    "            transforms.append(WPTE())\n",
    "            worker_transform.append(WPTE())\n",
    "        elif name == \"psd\":\n",
    "            transforms.append(PSD())\n",
    "            worker_transform.append(PSD())\n",
    "\n",
    "    transforms.append(FlattenChannel())\n",
    "    transforms.append(ConcatenateWorker(worker_transform))\n",
    "\n",
    "    # transforms.append(ToTensor(device=torch.device(\"cpu\")))\n",
    "\n",
    "    return Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# /Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import ClassifierMixin\n",
    "\n",
    "\n",
    "class BatchVotingClassifier(ClassifierMixin):\n",
    "    def __init__(self, model, voting=\"hard\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.voting = voting\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        X = np.vstack([np.vstack(batch) for batch in x])\n",
    "        Y = np.array([y[i] for i, batch in enumerate(x) for j in range(len(batch))])\n",
    "\n",
    "        self.model.fit(X, Y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        res_maj = np.zeros((len(X),))\n",
    "\n",
    "        for i, batch in enumerate(X):\n",
    "            batch = np.vstack(batch)\n",
    "            # code chunk from sklearn:\n",
    "            # https://github.com/scikit-learn/scikit-learn/blob/7e1e6d09bcc2eaeba98f7e737aac2ac782f0e5f1/sklearn/ensemble/_voting.py#L340\n",
    "            if self.voting == \"soft\":\n",
    "                maj = np.argmax(self.model.predict_proba(batch).reshape(-1, 1), axis=1)\n",
    "\n",
    "            else:  # 'hard' voting\n",
    "                predictions = self.model.predict(batch)\n",
    "                predictions = predictions.reshape(1, -1)\n",
    "                maj = np.apply_along_axis(\n",
    "                    lambda x: np.argmax(np.bincount(x)),\n",
    "                    axis=1,\n",
    "                    arr=predictions,\n",
    "                )\n",
    "            res_maj[i] = maj\n",
    "\n",
    "        return res_maj\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_eval(clf, X, Y, train_idx, test_idx, dataset):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    Y_train, Y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "    print(f\"The data shape : {X_train.shape}\")\n",
    "\n",
    "    print(\"*** Training the Model...\")\n",
    "    clf.fit(X_train, Y_train)\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # classes = list(map(str, dataset.classes))\n",
    "\n",
    "    print(\"Prediction on Train data\")\n",
    "    preds = clf.predict(X_train)\n",
    "    print(\"Done.\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            Y_train,\n",
    "            preds,\n",
    "            # labels=[i for i in range(len(classes))],\n",
    "            # target_names=classes,\n",
    "            zero_division=0,\n",
    "        )\n",
    "    )\n",
    "    temp_log = {}\n",
    "    temp_log[\"train accuracy\"] = accuracy_score(preds, Y_train)\n",
    "    temp_log[\"train F1 (micro)\"] = f1_score(preds, Y_train, average=\"micro\")\n",
    "    temp_log[\"train F1 (macro)\"] = f1_score(preds, Y_train, average=\"macro\")\n",
    "\n",
    "    print(\"Prediction on Test data\")\n",
    "    preds = clf.predict(X_test)\n",
    "    print(\"Done.\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            Y_test,\n",
    "            preds,\n",
    "            # labels=[i for i in range(len(classes))],\n",
    "            # target_names=classes,\n",
    "            zero_division=0,\n",
    "        )\n",
    "    )\n",
    "    temp_log[\"test accuracy\"] = accuracy_score(preds, Y_test)\n",
    "    temp_log[\"test F1 (micro)\"] = f1_score(preds, Y_test, average=\"micro\")\n",
    "    temp_log[\"test F1 (macro)\"] = f1_score(preds, Y_test, average=\"macro\")\n",
    "\n",
    "    if len(temp_log) > 0:\n",
    "        wandb.log(temp_log)\n",
    "\n",
    "\n",
    "# wandb.sklearn.plot_confusion_matrix(Y_test, preds, classes)\n",
    "# wandb.sklearn.plot_summary_metrics(clf, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splitter(splitter=\"\", n_splits=None):\n",
    "    cv = getattr(model_selection, splitter)\n",
    "\n",
    "    if splitter == \"train_test_split\":\n",
    "\n",
    "        def wrapper(x, y, group):\n",
    "            print(f\"X : {len(x)}\")\n",
    "            print(f\" Y : {len(y)}\")\n",
    "            print(f\"Cross Validation Type : {type(cv)}\")\n",
    "\n",
    "            train_idx, test_idx = cv(\n",
    "                x,\n",
    "                stratify=y,\n",
    "                test_size=0.2,\n",
    "            )\n",
    "\n",
    "            yield train_idx, test_idx\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    if splitter in [\"LeaveOneGroupOut\"]:\n",
    "        return cv().split\n",
    "\n",
    "    if splitter in [\"GroupKFold\", \"StratifiedGroupKFold\", \"StratifiedKFold\"]:\n",
    "        return cv(n_splits=n_splits if n_splits is not None else 4).split\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def select_dataset(opts, transform=None):\n",
    "    if opts[\"dataset\"] == \"KlinikDataset\":\n",
    "        dataset = KlinikDataset(\n",
    "            eeg_electrode_positions=opts[\"eeg_electrode_positions\"],\n",
    "            data_path=opts[\"data_path\"],\n",
    "            meta_data=None,\n",
    "            length=opts[\"length\"],\n",
    "            transforms=transform,\n",
    "        )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def select_model(opts):\n",
    "    if opts[\"model\"] == \"svm\":\n",
    "        clf = svm.SVC(decision_function_shape=\"ovo\", probability=True)\n",
    "    elif opts[\"model\"] == \"BalancedRandomForestClassifier\":\n",
    "        clf = BalancedRandomForestClassifier(\n",
    "            n_estimators=100, max_depth=4, n_jobs=os.cpu_count()\n",
    "        )\n",
    "    elif opts[\"model\"] == \"XGBoost\":\n",
    "        params = opts.copy()\n",
    "        params.pop(\"model\", None)\n",
    "        clf = XGBClassifier(use_label_encoder=False, **params)\n",
    "    elif opts[\"model\"] == \"GaussianNaiveBayes\":\n",
    "        clf = GaussianNB()\n",
    "    elif opts[\"model\"] == \"K-NearestNeighbores\":\n",
    "        clf = KNeighborsClassifier()\n",
    "    else:\n",
    "        print(f\"unknown model type {opts['model']}\")\n",
    "        clf = None\n",
    "\n",
    "    return clf\n",
    "\n",
    "\n",
    "def extract_sample_and_transform(args):\n",
    "    dataset, idx = args\n",
    "    sample_w, sample_l = dataset[idx]\n",
    "\n",
    "    return sample_w, sample_l, idx\n",
    "\n",
    "\n",
    "def find_nan_idx(x):\n",
    "    nan_idx = np.argwhere(np.isnan(x))\n",
    "    trials = np.array(list(set([i[0] for i in nan_idx])))\n",
    "    mask_trials = np.zeros(x.shape[0], dtype=bool)\n",
    "    mask_trials[trials] = True\n",
    "\n",
    "    return mask_trials\n",
    "\n",
    "\n",
    "def find_inf_idx(x):\n",
    "    inf_idx = np.argwhere(np.isinf(x))\n",
    "    trials = np.array(list(set([i[0] for i in inf_idx])))\n",
    "    print(f\"{trials = }\")\n",
    "    mask_trials = np.zeros(x.shape[0], dtype=bool)\n",
    "    mask_trials[trials] = True\n",
    "\n",
    "    return mask_trials\n",
    "\n",
    "\n",
    "def prepare_data(dataset, extract_sample_and_transform, name=\"data\"):\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    if not using_dask:\n",
    "        for i in tqdm(range(len(dataset))):\n",
    "            sample_w, sample_l, _ = extract_sample_and_transform((dataset, i))\n",
    "            X.append(sample_l[\"concat\"])\n",
    "            Y.append(sample_l[\"label\"])\n",
    "\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "\n",
    "        # Delete Nan from data\n",
    "        if np.isnan(np.sum(X)):\n",
    "            mask = find_nan_idx(X)\n",
    "            print(f\"mask true nan: {mask[mask == True].shape}\")\n",
    "            X = X[~mask]\n",
    "            Y = Y[~mask]\n",
    "\n",
    "        # Delete inf from data\n",
    "        if np.isinf(np.sum(X)):\n",
    "            mask = find_inf_idx(X)\n",
    "            print(f\"mask true inf: {mask[mask == True].shape}\")\n",
    "            X = X[~mask]\n",
    "            Y = Y[~mask]\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def train(opts):\n",
    "    transform = prepare_transforms(opts[\"workers\"])\n",
    "    dataset = select_dataset(opts, transform=transform)\n",
    "    print(f\"Dataset : {dataset}, Transformer : {transform}\")\n",
    "\n",
    "    classes = np.array([dataset.get_class(i) for i in range(len(dataset))])\n",
    "\n",
    "    groups = np.array([dataset.get_class(i) for i in range(len(dataset))])\n",
    "\n",
    "    X, Y = prepare_data(\n",
    "        dataset,\n",
    "        extract_sample_and_transform,\n",
    "        # name=f\"{opts['transform']}_{opts['dataset']}\",\n",
    "    )\n",
    "\n",
    "    print(f\"X: {X.shape}, Y : {Y.shape}\")\n",
    "\n",
    "    for train_idx, test_idx in get_splitter(\n",
    "        splitter=opts[\"data_splitter\"], n_splits=opts[\"n_splits\"]\n",
    "    )(list(range(len(X))), Y, Y):\n",
    "\n",
    "        wandb_run = wandb.init(\n",
    "            project=opts[\"proj\"], group=opts[\"wb_group\"], reinit=True\n",
    "        )\n",
    "        wandb.config.update(opts)\n",
    "\n",
    "        print(f\"Train Samples : {len(train_idx)}\")\n",
    "        print(f\"Test Samples : {len(test_idx)}\")\n",
    "\n",
    "        clf = select_model(opts)\n",
    "        print(f\"Model : {clf}\")\n",
    "        # clf = BatchVotingClassifier(clf)\n",
    "        fit_eval(clf, X, Y, train_idx, test_idx, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /Config\n",
    "\n",
    "eeg_electrode_positions = {\n",
    "    \"Fp1\": (-1, -2),\n",
    "    \"Fp2\": (1, -2),\n",
    "    \"F3\": (-1, -1),\n",
    "    \"F4\": (1, -1),\n",
    "    \"C3\": (-1, 0),\n",
    "    \"C4\": (1, 0),\n",
    "    \"P3\": (-1, 1),\n",
    "    \"P4\": (1, 1),\n",
    "    \"O1\": (-1, 2),\n",
    "    \"O2\": (1, 2),\n",
    "    \"F7\": (-2, -1),\n",
    "    \"F8\": (2, -1),\n",
    "    \"T3\": (-2, 0),\n",
    "    \"T4\": (2, 0),\n",
    "    \"T5\": (-2, 1),\n",
    "    \"T6\": (2, 1),\n",
    "    \"A1\": (-3, 0),\n",
    "    \"A2\": (3, 0),\n",
    "    \"Fz\": (0, -1),\n",
    "    \"Cz\": (0, 0),\n",
    "    \"Pz\": (0, 1),\n",
    "}\n",
    "\n",
    "eeg_electrods_plane_shape = (5, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\n",
    "    \"dataset\": \"KlinikDataset\",\n",
    "    \"eeg_electrode_positions\": eeg_electrode_positions,\n",
    "    \"data_path\": \"/data/\",\n",
    "    \"length\": 1,\n",
    "    \"n_splits\": 5,\n",
    "    \"proj\": \"baseline_models_eeg\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    \"transforms\": [\n",
    "        {\n",
    "            # \"workers\": [{\"name\": \"psd\"}, {\"name\": \"wte\"}],\n",
    "            \"workers\": [{\"name\": \"psd\"}, {\"name\": \"wte\"}],\n",
    "        },\n",
    "        # {\n",
    "        #     \"transform\": \"PASE_orig_pt\",\n",
    "        #     \"fe_cfg\": \"/usr/src/app/cfg/frontend/PASE+.cfg\",\n",
    "        #     \"fe_ckpt\": \"/experiments/pase_original_chpt/FE_e199.ckpt\",\n",
    "        # },\n",
    "        # {\"transform\": \"scat\"},\n",
    "    ],\n",
    "    \"models\": [\n",
    "        {\"model\": \"BalancedRandomForestClassifier\"},\n",
    "        {\"model\": \"XGBoost\", \"objective\": \"binary:logistic\"},  # \"multi:softprob\"\n",
    "        {\"model\": \"GaussianNaiveBayes\"},\n",
    "        {\"model\": \"K-NearestNeighbores\"},\n",
    "        {\"model\": \"svm\"},\n",
    "    ],\n",
    "    \"data_splitters\": [{\"data_splitter\": \"train_test_split\"}],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset : KlinikDataset, Transformer : Compose(LabelToDict-PSD(sfreq=256, n_fft=256, n_overlap=128, n_per_seg=256)-WTE(level=4, wavelet=db1)-FlattenChannel-ConcatenateWorker(PSD(sfreq=256, n_fft=256, n_overlap=128, n_per_seg=256)-WTE(level=4, wavelet=db1)))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▏                                                                                                                                                               | 6/848 [00:00<00:15, 54.66it/s]/tmp/ipykernel_77283/2285214147.py:392: RuntimeWarning: divide by zero encountered in log10\n",
      "  data = (10 * np.log10(data * sfreq / n_fft)).flatten()\n",
      "/tmp/ipykernel_77283/2285214147.py:185: RuntimeWarning: invalid value encountered in true_divide\n",
      "  energy_vector = ps_wt / np.sum(ps_wt)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 848/848 [00:15<00:00, 55.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask true nan: (27,)\n",
      "trials = array([130, 518, 136, 648, 649, 779,  12, 792, 153, 666, 155, 286, 673,\n",
      "       546,  41, 425, 442, 444, 714, 216,  95, 224, 104, 616, 363, 368])\n",
      "mask true inf: (26,)\n",
      "X: (795, 21, 134), Y : (795,)\n",
      "X : 795\n",
      " Y : 795\n",
      "Cross Validation Type : <class 'function'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1mfdx4rh) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 85051... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">twilight-lake-2</strong>: <a href=\"https://wandb.ai/keylead-team/baseline_models_eeg/runs/1mfdx4rh\" target=\"_blank\">https://wandb.ai/keylead-team/baseline_models_eeg/runs/1mfdx4rh</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220523_120057-1mfdx4rh/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1mfdx4rh). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.16 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/keylead-team/baseline_models_eeg/runs/16b37cgc\" target=\"_blank\">charmed-sun-3</a></strong> to <a href=\"https://wandb.ai/keylead-team/baseline_models_eeg\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Samples : 636\n",
      "Test Samples : 159\n",
      "Model : BalancedRandomForestClassifier(max_depth=4, n_jobs=6)\n",
      "The data shape : (636, 2814)\n",
      "*** Training the Model...\n",
      "Done.\n",
      "Prediction on Train data\n",
      "Done.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98       314\n",
      "           1       0.96      0.99      0.98       322\n",
      "\n",
      "    accuracy                           0.98       636\n",
      "   macro avg       0.98      0.98      0.98       636\n",
      "weighted avg       0.98      0.98      0.98       636\n",
      "\n",
      "Prediction on Test data\n",
      "Done.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93        78\n",
      "           1       0.94      0.93      0.93        81\n",
      "\n",
      "    accuracy                           0.93       159\n",
      "   macro avg       0.93      0.93      0.93       159\n",
      "weighted avg       0.93      0.93      0.93       159\n",
      "\n",
      "Dataset : KlinikDataset, Transformer : Compose(LabelToDict-PSD(sfreq=256, n_fft=256, n_overlap=128, n_per_seg=256)-WTE(level=4, wavelet=db1)-FlattenChannel-ConcatenateWorker(PSD(sfreq=256, n_fft=256, n_overlap=128, n_per_seg=256)-WTE(level=4, wavelet=db1)))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                         | 0/848 [00:00<?, ?it/s]/tmp/ipykernel_77283/2285214147.py:392: RuntimeWarning: divide by zero encountered in log10\n",
      "  data = (10 * np.log10(data * sfreq / n_fft)).flatten()\n",
      "  8%|█████████████▌                                                                                                                                                  | 72/848 [00:01<00:14, 55.40it/s]/tmp/ipykernel_77283/2285214147.py:185: RuntimeWarning: invalid value encountered in true_divide\n",
      "  energy_vector = ps_wt / np.sum(ps_wt)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 848/848 [00:15<00:00, 56.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask true nan: (26,)\n",
      "trials = array([  1, 513, 641, 521, 147, 660, 149, 533, 285, 157, 286, 415, 672,\n",
      "       551, 174, 303, 306, 437, 188, 195,  73, 721, 219,  91, 228, 363,\n",
      "       626, 754, 247])\n",
      "mask true inf: (29,)\n",
      "X: (793, 21, 134), Y : (793,)\n",
      "X : 793\n",
      " Y : 793\n",
      "Cross Validation Type : <class 'function'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:16b37cgc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 85100... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test F1 (macro)</td><td>▁</td></tr><tr><td>test F1 (micro)</td><td>▁</td></tr><tr><td>test accuracy</td><td>▁</td></tr><tr><td>train F1 (macro)</td><td>▁</td></tr><tr><td>train F1 (micro)</td><td>▁</td></tr><tr><td>train accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test F1 (macro)</td><td>0.93081</td></tr><tr><td>test F1 (micro)</td><td>0.93082</td></tr><tr><td>test accuracy</td><td>0.93082</td></tr><tr><td>train F1 (macro)</td><td>0.9764</td></tr><tr><td>train F1 (micro)</td><td>0.97642</td></tr><tr><td>train accuracy</td><td>0.97642</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">charmed-sun-3</strong>: <a href=\"https://wandb.ai/keylead-team/baseline_models_eeg/runs/16b37cgc\" target=\"_blank\">https://wandb.ai/keylead-team/baseline_models_eeg/runs/16b37cgc</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220523_120424-16b37cgc/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:16b37cgc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.16 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/keylead-team/baseline_models_eeg/runs/10q055bc\" target=\"_blank\">sunny-flower-4</a></strong> to <a href=\"https://wandb.ai/keylead-team/baseline_models_eeg\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Samples : 634\n",
      "Test Samples : 159\n",
      "Model : XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, data_path='/data/',\n",
      "              data_splitter='train_test_split', dataset='KlinikDataset',\n",
      "              eeg_electrode_positions={'A1': (-3, 0), 'A2': (3, 0),\n",
      "                                       'C3': (-1, 0), 'C4': (1, 0),\n",
      "                                       'Cz': (0, 0), 'F3': (-1, -1),\n",
      "                                       'F4': (1, -1), 'F7': (-2, -1),\n",
      "                                       'F8': (2, -1), 'Fp1': (-1, -2),\n",
      "                                       'Fp2': (1, -2), 'Fz':...\n",
      "              enable_categorical=False, gamma=None, gpu_id=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, length=1, max_delta_step=None, max_depth=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, n_splits=5, num_parallel_tree=None,\n",
      "              predictor=None, proj='baseline_models_eeg', random_state=None,\n",
      "              reg_alpha=None, reg_lambda=None, ...)\n",
      "The data shape : (634, 2814)\n",
      "*** Training the Model...\n",
      "[12:04:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"data_path\", \"data_splitter\", \"dataset\", \"eeg_electrode_positions\", \"length\", \"n_splits\", \"proj\", \"wb_group\", \"workers\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:04:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Done.\n",
      "Prediction on Train data\n",
      "Done.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       312\n",
      "           1       1.00      1.00      1.00       322\n",
      "\n",
      "    accuracy                           1.00       634\n",
      "   macro avg       1.00      1.00      1.00       634\n",
      "weighted avg       1.00      1.00      1.00       634\n",
      "\n",
      "Prediction on Test data\n",
      "Done.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99        78\n",
      "           1       0.99      0.99      0.99        81\n",
      "\n",
      "    accuracy                           0.99       159\n",
      "   macro avg       0.99      0.99      0.99       159\n",
      "weighted avg       0.99      0.99      0.99       159\n",
      "\n",
      "Dataset : KlinikDataset, Transformer : Compose(LabelToDict-PSD(sfreq=256, n_fft=256, n_overlap=128, n_per_seg=256)-WTE(level=4, wavelet=db1)-FlattenChannel-ConcatenateWorker(PSD(sfreq=256, n_fft=256, n_overlap=128, n_per_seg=256)-WTE(level=4, wavelet=db1)))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                         | 0/848 [00:00<?, ?it/s]/tmp/ipykernel_77283/2285214147.py:392: RuntimeWarning: divide by zero encountered in log10\n",
      "  data = (10 * np.log10(data * sfreq / n_fft)).flatten()\n",
      "  1%|██▎                                                                                                                                                             | 12/848 [00:00<00:14, 55.84it/s]/tmp/ipykernel_77283/2285214147.py:185: RuntimeWarning: invalid value encountered in true_divide\n",
      "  energy_vector = ps_wt / np.sum(ps_wt)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 848/848 [00:15<00:00, 55.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask true nan: (29,)\n",
      "trials = array([130,   5, 393, 522, 396, 151, 797, 422, 554, 813, 304, 560, 435,\n",
      "        59,  61, 710, 457, 338, 471, 347, 356, 490, 116, 633])\n",
      "mask true inf: (24,)\n",
      "X: (795, 21, 134), Y : (795,)\n",
      "X : 795\n",
      " Y : 795\n",
      "Cross Validation Type : <class 'function'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:10q055bc) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 85170... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test F1 (macro)</td><td>▁</td></tr><tr><td>test F1 (micro)</td><td>▁</td></tr><tr><td>test accuracy</td><td>▁</td></tr><tr><td>train F1 (macro)</td><td>▁</td></tr><tr><td>train F1 (micro)</td><td>▁</td></tr><tr><td>train accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test F1 (macro)</td><td>0.98742</td></tr><tr><td>test F1 (micro)</td><td>0.98742</td></tr><tr><td>test accuracy</td><td>0.98742</td></tr><tr><td>train F1 (macro)</td><td>1.0</td></tr><tr><td>train F1 (micro)</td><td>1.0</td></tr><tr><td>train accuracy</td><td>1.0</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sunny-flower-4</strong>: <a href=\"https://wandb.ai/keylead-team/baseline_models_eeg/runs/10q055bc\" target=\"_blank\">https://wandb.ai/keylead-team/baseline_models_eeg/runs/10q055bc</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220523_120448-10q055bc/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:10q055bc). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.16 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/keylead-team/baseline_models_eeg/runs/w2iwz5u9\" target=\"_blank\">light-snow-5</a></strong> to <a href=\"https://wandb.ai/keylead-team/baseline_models_eeg\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Samples : 636\n",
      "Test Samples : 159\n",
      "Model : GaussianNB()\n",
      "The data shape : (636, 2814)\n",
      "*** Training the Model...\n",
      "Done.\n",
      "Prediction on Train data\n",
      "Done.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.93      0.83       314\n",
      "           1       0.91      0.71      0.80       322\n",
      "\n",
      "    accuracy                           0.82       636\n",
      "   macro avg       0.83      0.82      0.82       636\n",
      "weighted avg       0.83      0.82      0.82       636\n",
      "\n",
      "Prediction on Test data\n",
      "Done.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86        78\n",
      "           1       0.93      0.77      0.84        81\n",
      "\n",
      "    accuracy                           0.85       159\n",
      "   macro avg       0.86      0.85      0.85       159\n",
      "weighted avg       0.86      0.85      0.85       159\n",
      "\n",
      "Dataset : KlinikDataset, Transformer : Compose(LabelToDict-PSD(sfreq=256, n_fft=256, n_overlap=128, n_per_seg=256)-WTE(level=4, wavelet=db1)-FlattenChannel-ConcatenateWorker(PSD(sfreq=256, n_fft=256, n_overlap=128, n_per_seg=256)-WTE(level=4, wavelet=db1)))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|██▎                                                                                                                                                             | 12/848 [00:00<00:14, 56.75it/s]/tmp/ipykernel_77283/2285214147.py:392: RuntimeWarning: divide by zero encountered in log10\n",
      "  data = (10 * np.log10(data * sfreq / n_fft)).flatten()\n",
      "/tmp/ipykernel_77283/2285214147.py:185: RuntimeWarning: invalid value encountered in true_divide\n",
      "  energy_vector = ps_wt / np.sum(ps_wt)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 848/848 [00:15<00:00, 55.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask true nan: (26,)\n",
      "trials = array([512, 388, 637, 141, 399,  19, 284, 159, 423,  48,  54, 698, 443,\n",
      "       316, 324, 709, 582, 461, 208, 473, 346, 606, 631, 110, 371, 372,\n",
      "       119, 509])\n",
      "mask true inf: (28,)\n",
      "X: (794, 21, 134), Y : (794,)\n",
      "X : 794\n",
      " Y : 794\n",
      "Cross Validation Type : <class 'function'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:w2iwz5u9) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 85213... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test F1 (macro)</td><td>▁</td></tr><tr><td>test F1 (micro)</td><td>▁</td></tr><tr><td>test accuracy</td><td>▁</td></tr><tr><td>train F1 (macro)</td><td>▁</td></tr><tr><td>train F1 (micro)</td><td>▁</td></tr><tr><td>train accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test F1 (macro)</td><td>0.84833</td></tr><tr><td>test F1 (micro)</td><td>0.84906</td></tr><tr><td>test accuracy</td><td>0.84906</td></tr><tr><td>train F1 (macro)</td><td>0.81586</td></tr><tr><td>train F1 (micro)</td><td>0.81761</td></tr><tr><td>train accuracy</td><td>0.81761</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">light-snow-5</strong>: <a href=\"https://wandb.ai/keylead-team/baseline_models_eeg/runs/w2iwz5u9\" target=\"_blank\">https://wandb.ai/keylead-team/baseline_models_eeg/runs/w2iwz5u9</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220523_120514-w2iwz5u9/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:w2iwz5u9). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.16 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/keylead-team/baseline_models_eeg/runs/36ar7jrh\" target=\"_blank\">flowing-donkey-6</a></strong> to <a href=\"https://wandb.ai/keylead-team/baseline_models_eeg\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Samples : 635\n",
      "Test Samples : 159\n",
      "Model : KNeighborsClassifier()\n",
      "The data shape : (635, 2814)\n",
      "*** Training the Model...\n",
      "Done.\n",
      "Prediction on Train data\n",
      "Done.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.97       313\n",
      "           1       0.94      1.00      0.97       322\n",
      "\n",
      "    accuracy                           0.97       635\n",
      "   macro avg       0.97      0.97      0.97       635\n",
      "weighted avg       0.97      0.97      0.97       635\n",
      "\n",
      "Prediction on Test data\n",
      "Done.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        78\n",
      "           1       0.93      1.00      0.96        81\n",
      "\n",
      "    accuracy                           0.96       159\n",
      "   macro avg       0.97      0.96      0.96       159\n",
      "weighted avg       0.96      0.96      0.96       159\n",
      "\n",
      "Dataset : KlinikDataset, Transformer : Compose(LabelToDict-PSD(sfreq=256, n_fft=256, n_overlap=128, n_per_seg=256)-WTE(level=4, wavelet=db1)-FlattenChannel-ConcatenateWorker(PSD(sfreq=256, n_fft=256, n_overlap=128, n_per_seg=256)-WTE(level=4, wavelet=db1)))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▏                                                                                                                                                               | 6/848 [00:00<00:14, 56.23it/s]/tmp/ipykernel_77283/2285214147.py:392: RuntimeWarning: divide by zero encountered in log10\n",
      "  data = (10 * np.log10(data * sfreq / n_fft)).flatten()\n",
      "/tmp/ipykernel_77283/2285214147.py:185: RuntimeWarning: invalid value encountered in true_divide\n",
      "  energy_vector = ps_wt / np.sum(ps_wt)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 848/848 [00:14<00:00, 56.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask true nan: (33,)\n",
      "trials = array([260, 134, 135, 523, 273, 402,  19, 538, 411, 156,  29, 670, 287,\n",
      "       794, 551, 174, 433, 693, 704, 321, 582, 466,  94, 734, 103, 752,\n",
      "       242, 373, 374, 249, 507, 126])\n",
      "mask true inf: (32,)\n",
      "X: (783, 21, 134), Y : (783,)\n",
      "X : 783\n",
      " Y : 783\n",
      "Cross Validation Type : <class 'function'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:36ar7jrh) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 85256... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test F1 (macro)</td><td>▁</td></tr><tr><td>test F1 (micro)</td><td>▁</td></tr><tr><td>test accuracy</td><td>▁</td></tr><tr><td>train F1 (macro)</td><td>▁</td></tr><tr><td>train F1 (micro)</td><td>▁</td></tr><tr><td>train accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test F1 (macro)</td><td>0.96214</td></tr><tr><td>test F1 (micro)</td><td>0.96226</td></tr><tr><td>test accuracy</td><td>0.96226</td></tr><tr><td>train F1 (macro)</td><td>0.96686</td></tr><tr><td>train F1 (micro)</td><td>0.96693</td></tr><tr><td>train accuracy</td><td>0.96693</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">flowing-donkey-6</strong>: <a href=\"https://wandb.ai/keylead-team/baseline_models_eeg/runs/36ar7jrh\" target=\"_blank\">https://wandb.ai/keylead-team/baseline_models_eeg/runs/36ar7jrh</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220523_120539-36ar7jrh/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:36ar7jrh). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.16 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/keylead-team/baseline_models_eeg/runs/3gu55wal\" target=\"_blank\">dark-vortex-7</a></strong> to <a href=\"https://wandb.ai/keylead-team/baseline_models_eeg\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Samples : 626\n",
      "Test Samples : 157\n",
      "Model : SVC(decision_function_shape='ovo', probability=True)\n",
      "The data shape : (626, 2814)\n",
      "*** Training the Model...\n",
      "Done.\n",
      "Prediction on Train data\n",
      "Done.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94       304\n",
      "           1       0.93      0.97      0.95       322\n",
      "\n",
      "    accuracy                           0.94       626\n",
      "   macro avg       0.94      0.94      0.94       626\n",
      "weighted avg       0.94      0.94      0.94       626\n",
      "\n",
      "Prediction on Test data\n",
      "Done.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91        76\n",
      "           1       0.94      0.89      0.91        81\n",
      "\n",
      "    accuracy                           0.91       157\n",
      "   macro avg       0.91      0.91      0.91       157\n",
      "weighted avg       0.91      0.91      0.91       157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model, transform, splitter in itertools.product(\n",
    "    configs[\"models\"], configs[\"transforms\"], configs[\"data_splitters\"]\n",
    "):\n",
    "\n",
    "    temp_conf = conf.copy()\n",
    "    temp_conf.update(model)\n",
    "    temp_conf.update(transform)\n",
    "    temp_conf.update(splitter)\n",
    "    temp_conf[\n",
    "        \"wb_group\"\n",
    "    ] = f\"{model['model']}_{'_'.join(worker['name'] for worker in transform['workers'])}_{splitter['data_splitter']}\"\n",
    "\n",
    "    # print(temp_conf)\n",
    "\n",
    "    train(temp_conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
